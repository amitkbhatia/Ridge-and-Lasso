---
title: "Hitters_Lasso"
author: "Amit"
date: "February 26, 2017"
output:  rmarkdown::github_document
---
```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ridge Regression and Lasso Regression
-------------------------------------
I will use the package `glmnet`.There are some missing values here, so before proceeding I will remove them:
Lets make a training and validation set

```{r warning=FALSE}
library(ISLR)
library(glmnet)
library(plotmo)
```
Hitters dataset-Major League Baseball Data from the 1986 and 1987 seasons
For description of dataset, please refer -https://cran.r-project.org/web/packages/ISLR/ISLR.pdf
```{r warning=FALSE}

summary(Hitters)
dim(Hitters)
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```

```{r}
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
```

```{r}
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters) 
y=Hitters$Salary
```
Fit a ridge-regression model. 

This can be done by calling `glmnet` with `alpha=0` 
```{r}
fit.ridge=glmnet(x,y,alpha=0)
par(mfrow=c(1,2))
plot_glmnet(fit.ridge,xvar="lambda",label=5)

plot_glmnet(fit.ridge,label=5)

cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

To fit a lasso model; `alpha=1`
```{r}
fit.lasso=glmnet(x,y)
par(mfrow=c(1,2))
plot_glmnet(fit.lasso,xvar="lambda",label=5)
plot_glmnet(fit.lasso,label=5)
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso, label=5)
coef(cv.lasso)
```

Alternative approach- selection of `lambda` using our earlier train/validation division for the lasso.
```{r}
lasso.tr=glmnet(x[train,],y[train])
pred=predict(lasso.tr,x[-train,])
dim(pred)
rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
```
